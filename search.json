[
  {
    "objectID": "grel.html",
    "href": "grel.html",
    "title": "More Transformations with GREL",
    "section": "",
    "text": "Now that we‚Äôre familiar with common transformations using the menu‚Äôs preset options, let‚Äôs take a step further by exploring how to customize transformations with more advanced features.\nIn OpenRefine, expressions can be used in various areas to enhance data cleanup and transformation. You can apply expressions in the following functions:\nFacets:\n\nCustom text facet\nCustom numeric facet\nCustomized facets (click ‚ÄúChange‚Äù after creating them to open the expressions window)\n\nEdit Cells:\n\nTransform\nSplit multi-valued cells\nJoin multi-valued cells\n\nEdit Column:\n\nSplit\nJoin\nAdd column based on this column\nAdd column by fetching URLs\n\nAny of these options should prompt the Expressions Editor window as shown below:\n\nIn the Expressions Editor window, you can choose from several supported languages. By default, it uses GREL (General Refine Expression Language), OpenRefine also supports Clojure and Jython, these lasts represent Lisp and Python implementations for Java, which makes sense considering OpenRefine is java-based application.\nAdditional expression languages may be available through extensions. While these languages differ in syntax, they support many of the same variables and functions. For example, the GREL expression value.split(\" \")[1] is written in Jython as return value.split(\" \")[1]. But they may also look a lot more different. For example, the GREL function toUppercase(value) would be expressed in Clojure as (clojure.string/upper-case value).\nWe‚Äôll now look at some practical examples that are relevant to our sample dataset. These will give you a sense of what‚Äôs possible‚Äîbut since there‚Äôs so much more you can do, we encourage you to explore further using the Recipe for more detailed information and hands-on examples. It‚Äôs a great resource to help you experiment further at your own pace.\n\nCustom Data Transforms with GREL\nLet‚Äôs start by focusing on the imdb_score and tmdb_score columns. These columns contain floating-point numbers, right? To confirm, we can run a text facet to eyeball those values. As we examine the values, we‚Äôll notice that tmdb scores don‚Äôt follow the one decimal format as the imdb. in the number of decimal places. You might be thinking, ‚ÄúThis would be such an easy fix in Excel‚Äîthere‚Äôs literally a button for it on the toolbar. Why bother using OpenRefine for that?‚Äù\nThat‚Äôs a fair point‚ÄîExcel does make it easy to change how numbers look by formatting decimal places. But here‚Äôs the catch: Excel typically changes the display of the number, not the actual underlying value. So a number like 7.456 may appear as 7.5, but it‚Äôs still stored as 7.456, which can cause inconsistencies in downstream processes like sorting, filtering, or exporting.\nOpenRefine, on the other hand, lets you transform the actual data. You can standardize the number of decimal places across all values using expressions in GREL. That way, you‚Äôre not just changing how the data looks‚Äîyou‚Äôre ensuring consistency at the data level, which is critical for clean, reliable datasets.\nAlso, OpenRefine is designed for handling large datasets with repeatable, auditable transformations. So once you clean up one column, you can apply the same steps to others, or even to other projects. That‚Äôs something Excel just isn‚Äôt built for.\n\nAdjusting Decimal Points\nIn order to adjust tmdb scores values, we will need to use an expression. Can you guess which one?\nLet‚Äôs try round(value * 10) / 10.0 which will essentially round the value to the nearest integer and then take this whole number and shift the decimal point back. Apply that, then check how many cells were modified. But wait‚Ä¶have you remembered to convert the values to numbers? That is a key step, since the round function only applies to numbers!\nBefore applying the expression OpenRefine gives you a hint in case there are any syntax errors and how values will look like if you proceed. You may also choose the behavior in case some rows\n\nYou may also choose how the system should behave if any error occurs during the transform:\n\nKeep original: Leave the original value if an error occurs.\nSet to blank: Replace the value with a blank if an error occurs.\nStore error: Display an error message in the cell if an error occurs.\n\nIn this particular case, we chose to retain the original values because we had already verified that the cells contain only numeric data. Otherwise, any blank cells would have resulted in errors.\n\n\nReusing Expressions\nAnother great feature of OpenRefine is the ability to easily reuse expressions within the same project or across different ones. The History tab records all the expressions you‚Äôve written.\n\nApparently, there‚Äôs no limit to how many are stored, so it may accumulate expressions you didn‚Äôt end up using or had to revise. For expressions that worked well, consider starring them to make future reuse easier. However, note that the history is cleared when you upgrade to a new version of OpenRefine. The Help tab is also handy for quick references on how to write expressions.\n\n\nDealing with Arrays\nNow that we‚Äôve completed a basic custom transformation and gained a better understanding of how GREL works, let‚Äôs turn our attention back to the country column. Earlier, we corrected entries where ‚ÄúUSA‚Äù was used instead of ‚ÄúUS‚Äù to ensure that all country codes follow the two-character format defined by the ISO.\nBy applying a text facet, we will notice that there are over 1,030 unique values. This high number is due to OpenRefine treating different sequences of multiple countries as distinct entries. Although if we were to check country representation within our dataset, this would be challenging because the country information is currently stored as arrays.\nAn ‚ÄòArray‚Äô is a data type which can contain a list of values. In OpenRefine an array is represented by the use of square brackets containing a list of values separated by commas.\nArrays can be sorted, de-duplicated, and manipulated in other ways in GREL expressions, but cannot be stored directly in an OpenRefine cell. Arrays in OpenRefine are usually the result of a transformation written with GREL. For example the split function takes a string, and changes it into an array based on a ‚Äòseparator‚Äô.\nIn OpenRefine, cells are designed to hold text values‚Äîtypically strings or single pieces of other data types. Arrays, which are structured collections of multiple values, cannot appear directly in a cell as an actual data type. Instead, if a cell appears to contain multiple items (like a list of countries), it‚Äôs usually represented as a string with a delimiter, here expressed as commas.\nTo work with these values individually, you often need to split the string into separate parts using a transformation function like split(), which temporarily treats the cell content as an array for processing purposes.\nLet‚Äôs break down the approach for transforming the data, starting with replacements and then splitting arrays into individual elements. Our goal is to organize the information in a way that makes it easier to identify the top three countries leading the most productions in our dataset.\nKeep in mind that for rows listing two or more countries, the countries are ordered by their significance in the production‚Äînot alphabetically.\nWhile arrays are not necessarily messy data, we can apply GREL to separate pieces of information to facilitate data analysis. But first, let‚Äôs ensure we make a copy of the country column and name it countries.\n\n\n\n\n\n\n\nFrom now on, we will continue working with the ‚Äúcountry‚Äù column and live ‚Äúcountries‚Äù with the array. For the column ‚Äúcountry‚Äù our task will be to extract only the first value in the array.\nFirst, let‚Äôs ensure we remove the both square brackets‚Ä¶\nvalue.replace('[', \"\").replace(']', \"\")\nBut wait, we still have single quotes to remove‚Ä¶so let‚Äôs try:\nvalue.replace(\"'\", \"\")\nAnd finally, we would ask to only keep the first country listed.\nvalue.split(\",\")[0]\nThat‚Äôs right! The first position starts at 0 not 1.\n\n\n\n\n\n\nPutting everything together\n\n\n\n\n\nWe employed a three-step approach to accomplish that, though these steps could also be consolidated into a single, more streamlined expression.\nvalue.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(\",\")[0]\n\n\n\nBased on this new column, which three countries lead in production across the streaming services in our dataset? If you answered the USA (US), India (IN), and Great Britain (GB), you‚Äôre absolutely right!\n\n\n\n\n\n\nüß† Your Turn!\n\n\n\n\n\nNow, let‚Äôs perform a similar transformation on the genre column and answer? What are the three most common leading genres in this dataset?\n\n\n\n\n\n\nüîë Answer Key\n\n\n\n\n\nIf you were paying close attention to the earlier tip, this should‚Äôve been a breeze! That‚Äôs because there was no need to retype the expression‚Äîyou could simply reuse it. As a result, the top three genres are Drama, Comedy, and Documentary.\n\n\n\n\n\n\nBefore we move on to the next episode, there‚Äôs one more important step to cover: data documentation. Since we‚Äôve added new columns to our dataset, it‚Äôs essential that we update our documentation accordingly. This ensures that anyone working with the data‚Äînow or in the future‚Äîcan easily understand what each column represents, how it was created, and why it‚Äôs relevant. Proper documentation keeps your project organized, reproducible, and easier to maintain.\nThankfully, OpenRefine supports reproducibility by keeping a detailed history of every transformation you apply‚Äîmaking it easy to trace, review, or reapply steps. In the next episode, we‚Äôll explore how to export and share your data cleaning workflow so that anyone can reproduce the exact same process with just a few clicks.",
    "crumbs": [
      "More Transformations with GREL"
    ]
  },
  {
    "objectID": "dataset.html",
    "href": "dataset.html",
    "title": "Our Running Example",
    "section": "",
    "text": "This workshop utilizes the streaming-master-messy comma-separated value (CSV) file about movie and TV shows productions featured by major streaming services and distributed in Kaggle Project under a CC0 Public License:\nHenrique, D. (2020). A simple movie & TV show recommendation system. Kaggle. https://www.kaggle.com/code/dgoenrique/a-simple-movie-tv-show-recommendation-system?select=credits.csv\nWe have combined six datasets titles.csv files representing each of the streaming services featured in this project (Amazon Prime Video, Apple TV+, Disney+, HBO Max, Netflix, and Paramount) into a single master file.\nFor the purposes of this lesson, the data has been intentionally modified to support the exercises.\nThe dataset contains 25,223 rows with movies and TV series titles along with the following variables as described in the data dictionary:\n\nstreaming: streaming service/provider\nid: unique movie or TV series ID\ntitle: movie or TV series title\ntype: if movie or tv series\ndescription: the synopsis/plot summary\nrelease_year: YYYY\nclassification: age group movie age categories including G (General Audiences), PG (Parental Guidance), PG-13 (Parents Strongly Cautioned), R (Restricted), and NC-17 (No One 17 and Under Admitted).\nruntime: total time in minutes\ngenre(s): the thematic classification that define the type of story, themes, and style of the production, by order of relevance.\ncountry: code for country or countries responsible for the production, ordered by importance.\nimdb_id: unique ID attributed to each production by IMDB.\nimdb_score: the rating on IMDB.\nseasons: Number of seasons if it‚Äôs a show.\nimdb_id: The title ID on IMDB.\nimdb_score: Score on the Internet Movie Database (IMDB).\nimdb_votes: Votes on the Internet Movie Database (IMDB).\ntmdb_popularity: Votes on The Movie Database (TMDB).\ntmdb_score: Score on on The Movie Database TMDB.\n\n\n\n\n\n\n\nDisclaimer\n\n\n\n\n\nPlease be advised that we do not vouch for the use of this dataset for actual research. The data has been specifically edited and curated for instructional purposes and may not represent a fully accurate or comprehensive source of data for formal analysis.",
    "crumbs": [
      "Our Running Example"
    ]
  },
  {
    "objectID": "dataset.html#streamed-movies-tv-shows-dataset",
    "href": "dataset.html#streamed-movies-tv-shows-dataset",
    "title": "Our Running Example",
    "section": "",
    "text": "This workshop utilizes the streaming-master-messy comma-separated value (CSV) file about movie and TV shows productions featured by major streaming services and distributed in Kaggle Project under a CC0 Public License:\nHenrique, D. (2020). A simple movie & TV show recommendation system. Kaggle. https://www.kaggle.com/code/dgoenrique/a-simple-movie-tv-show-recommendation-system?select=credits.csv\nWe have combined six datasets titles.csv files representing each of the streaming services featured in this project (Amazon Prime Video, Apple TV+, Disney+, HBO Max, Netflix, and Paramount) into a single master file.\nFor the purposes of this lesson, the data has been intentionally modified to support the exercises.\nThe dataset contains 25,223 rows with movies and TV series titles along with the following variables as described in the data dictionary:\n\nstreaming: streaming service/provider\nid: unique movie or TV series ID\ntitle: movie or TV series title\ntype: if movie or tv series\ndescription: the synopsis/plot summary\nrelease_year: YYYY\nclassification: age group movie age categories including G (General Audiences), PG (Parental Guidance), PG-13 (Parents Strongly Cautioned), R (Restricted), and NC-17 (No One 17 and Under Admitted).\nruntime: total time in minutes\ngenre(s): the thematic classification that define the type of story, themes, and style of the production, by order of relevance.\ncountry: code for country or countries responsible for the production, ordered by importance.\nimdb_id: unique ID attributed to each production by IMDB.\nimdb_score: the rating on IMDB.\nseasons: Number of seasons if it‚Äôs a show.\nimdb_id: The title ID on IMDB.\nimdb_score: Score on the Internet Movie Database (IMDB).\nimdb_votes: Votes on the Internet Movie Database (IMDB).\ntmdb_popularity: Votes on The Movie Database (TMDB).\ntmdb_score: Score on on The Movie Database TMDB.\n\n\n\n\n\n\n\nDisclaimer\n\n\n\n\n\nPlease be advised that we do not vouch for the use of this dataset for actual research. The data has been specifically edited and curated for instructional purposes and may not represent a fully accurate or comprehensive source of data for formal analysis.",
    "crumbs": [
      "Our Running Example"
    ]
  },
  {
    "objectID": "dataset.html#downloading-the-dataset",
    "href": "dataset.html#downloading-the-dataset",
    "title": "Our Running Example",
    "section": "Downloading the Dataset",
    "text": "Downloading the Dataset\nNow that we have a clearer understanding of the data we‚Äôll be working with, please take a moment to download and save the file to your working environment.\n\nFIXME: [add zenodo link to dataset]\nLet‚Äôs open the file and check how the data looks like. Also, can you spot your favorite movie or TV series on it?",
    "crumbs": [
      "Our Running Example"
    ]
  },
  {
    "objectID": "dataset.html#our-challenge",
    "href": "dataset.html#our-challenge",
    "title": "Our Running Example",
    "section": "Our Challenge",
    "text": "Our Challenge\nIn this workshop, we will explore how OpenRefine can support data organization and preparation for analysis. For instance, you might want to compare scores across genres, plot the most common age classifications over the years, or investigate whether the country of origin affects popularity. These are just a few examples of the kinds of insights you could uncover once your data is properly cleaned and organized. But before that the data has to be cleaned and prepared accordingly. Ready?",
    "crumbs": [
      "Our Running Example"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "License\nThis work is licensed under the Creative Commons Attribution 4.0 International License (CC BY). To attribute this work, please follow use the recommended citation listed at the bottom of the index.qmd page."
  },
  {
    "objectID": "setup.html#why-openrefine",
    "href": "setup.html#why-openrefine",
    "title": "Setup & Installation",
    "section": "Why OpenRefine?",
    "text": "Why OpenRefine?\nOpenRefine is a powerful open-source multiplatform and free data cleaning and transformation tool designed to work with large datasets. It helps users explore, clean, and refine data, making it easier to analyze and prepare for further use with no coding skills required.\nOne of the key strengths of OpenRefine is its ability to track every action you take through its project history feature. This means you can easily review, reverse, or revisit any step in your data cleaning process. It makes the entire workflow reproducible, as you can apply the same steps to other datasets or go back to previous stages without losing any progress.\nWhat really sets OpenRefine apart, though, is how it combines this project history with version control. This is especially useful when working in teams. As your project evolves, version control allows you to track changes over time, keeping everyone aligned and on the same page. If you‚Äôre working with multiple collaborators, you can rely on OpenRefine‚Äôs version history to ensure that everyone is using the same cleaning process and has access to the latest changes. This collaborative feature is a big advantage over other tools, which often don‚Äôt offer such detailed tracking or easy collaboration, making it harder to maintain consistency or manage multiple revisions.\nAdditionally, since OpenRefine is open-source, you can create and share custom extensions, further streamlining teamwork. The combination of project history and version control makes OpenRefine particularly powerful for teams, ensuring a smooth, consistent, and transparent workflow, all while reducing the risk of errors. Unlike other data cleaning tools, OpenRefine offers a level of collaboration and organization that helps keep your cleaning process standardized and your team on track.",
    "crumbs": [
      "Setup & Installation"
    ]
  },
  {
    "objectID": "setup.html#donwload-requirements",
    "href": "setup.html#donwload-requirements",
    "title": "Setup & Installation",
    "section": "Donwload & Requirements",
    "text": "Donwload & Requirements\nOpenRefine is designed to work with Windows, Mac, and Linux operating systems and can be downloaded from https://openrefine.org/download. To run OpenRefine, Java must be installed. The Mac version includes Java, and OpenRefine 3.4 or higher offers a Windows package with Java included. Alternatively, you can install Java (JRE) from Adoptium.net. On Windows, if Java isn‚Äôt installed, OpenRefine will automatically open a browser window with installation instructions.\n\n\n\n\n\n\nmacOS Verification\n\n\n\n\n\nWhen you first try to launch OpenRefine on a Mac by double-clicking its icon, you might see a message saying:\n\n‚ÄúOpenRefine cannot be opened because the developer cannot be verified.‚Äù\n\nIf this appears, click Cancel to dismiss the message.\nTo open the application anyway:\n\nRight-click (or Control-click) the OpenRefine icon.\nChoose Open from the context menu.\nA similar warning will appear, but this time it will include an Open button.\nClick Open to launch the application.\n\nmacOS will remember this choice, and you won‚Äôt need to repeat these steps the next time you open OpenRefine.\n\n\n\nIf you‚Äôre using the desktop version, double-click on the OpenRefine icon or run the executable. OpenRefine runs in the browser but it does not require internet access for basic functions. It works best on browsers using the engine WebKit, such as Chrome and Safari. No matter how you start OpenRefine, it will load its interface in your computer‚Äôs default browser.\n\n\n\n\n\n\nStarting OpenRefine Straight in Your Browser\n\n\n\n\n\nIf you would like to use another browser instead, start OpenRefine and then point your chosen browser at the home screen: http://127.0.0.1:3333.",
    "crumbs": [
      "Setup & Installation"
    ]
  },
  {
    "objectID": "filtering.html",
    "href": "filtering.html",
    "title": "Filtering & Sorting",
    "section": "",
    "text": "At times, you might need to isolate certain parts of your data or carry out tasks on only a specific segment. This can be done by using filters to selectively display the data that meets your criteria.\n\n\nOne way to filter down our data is to use the include or exclude buttons on the entries in a text facet. If you still have your text facet for title, you can use it. If you‚Äôve closed that facet, recreate it by selecting Facet &gt; Text facet on the title column and change from name to count.\nOne way to filter down our data is to use the include or exclude buttons on the entries in a text facet. If you still have your text facet for title, you can use it. If you‚Äôve closed that facet, recreate it by selecting Facet &gt; Text facet on the title column.\n\nIn the text facet, hover over one of the titles, e.g.¬†Hercules. Notice that when you hover over it, there are buttons to the right for edit and include.\nWhilst hovering over Hercules, move to the right and click the include option. This will include this title, changing from blue to orange, and new options of edit and exclude will be presented. Note that in the top of the page, ‚Äú5 matching rows‚Äù is now displayed instead of the thousand rows we had previously.\nYou can include other titles in your current filter - e.g.click on The Stranger in the same way to include it in the filter.\nAlternatively, you can click the name of the species to include it in the filter instead of clicking the include/exclude buttons. This will include the selected species and exclude all other options in a single step, which can be useful.\nClick include and exclude on the other species and notice how the entries appear and disappear from the data table to the right.\n\nClick on Reset at the top-right of the facet before continuing to the next step.\n\n\n\nOne way to filter data is to create a text filter on a column. Close all facets you may have created previously and reinstate the text facet on the title column.\n\nClick the down arrow next to title &gt; Text filter. A filter will appear on the left margin below the text facet.\nType in american into the text box in the filter and press return. At the top of the page it will report that, out of the total rows 123 where found and can be selected for the subsequent steps.",
    "crumbs": [
      "Filtering & Sorting"
    ]
  },
  {
    "objectID": "filtering.html#filtering",
    "href": "filtering.html#filtering",
    "title": "Filtering & Sorting",
    "section": "",
    "text": "At times, you might need to isolate certain parts of your data or carry out tasks on only a specific segment. This can be done by using filters to selectively display the data that meets your criteria.\n\n\nOne way to filter down our data is to use the include or exclude buttons on the entries in a text facet. If you still have your text facet for title, you can use it. If you‚Äôve closed that facet, recreate it by selecting Facet &gt; Text facet on the title column and change from name to count.\nOne way to filter down our data is to use the include or exclude buttons on the entries in a text facet. If you still have your text facet for title, you can use it. If you‚Äôve closed that facet, recreate it by selecting Facet &gt; Text facet on the title column.\n\nIn the text facet, hover over one of the titles, e.g.¬†Hercules. Notice that when you hover over it, there are buttons to the right for edit and include.\nWhilst hovering over Hercules, move to the right and click the include option. This will include this title, changing from blue to orange, and new options of edit and exclude will be presented. Note that in the top of the page, ‚Äú5 matching rows‚Äù is now displayed instead of the thousand rows we had previously.\nYou can include other titles in your current filter - e.g.click on The Stranger in the same way to include it in the filter.\nAlternatively, you can click the name of the species to include it in the filter instead of clicking the include/exclude buttons. This will include the selected species and exclude all other options in a single step, which can be useful.\nClick include and exclude on the other species and notice how the entries appear and disappear from the data table to the right.\n\nClick on Reset at the top-right of the facet before continuing to the next step.\n\n\n\nOne way to filter data is to create a text filter on a column. Close all facets you may have created previously and reinstate the text facet on the title column.\n\nClick the down arrow next to title &gt; Text filter. A filter will appear on the left margin below the text facet.\nType in american into the text box in the filter and press return. At the top of the page it will report that, out of the total rows 123 where found and can be selected for the subsequent steps.",
    "crumbs": [
      "Filtering & Sorting"
    ]
  },
  {
    "objectID": "filtering.html#sorting",
    "href": "filtering.html#sorting",
    "title": "Filtering & Sorting",
    "section": "Sorting",
    "text": "Sorting\nSorting data is a useful practice for detecting outliers in data - potential errors and blanks will sort to the top or the bottom of your data.\nYou can sort the data in a column by using the drop-down menu available in that column. There you can sort by text, numbers, dates or booleans (TRUE or FALSE values). You can also specify what order to put Blanks and Errors in the sorted results.\nIf this is your first time sorting this table, then the drop-down menu for the selected column shows Sort.... Select what you would like to sort by (such as numbers). Additional options will then appear for you to fine-tune your sorting.\nIf you try to re-sort a column that you have already used, the drop-down menu changes slightly, to &gt; Sort without the ..., to remind you that you have already used this column. It will give you additional options:\n\nAfter you apply a sorting method, you can make it permanent, remove it, reverse it, or apply a subsequent sorting. When it is applied, you‚Äôll find Sort in the project grid header to the right of the rows-display setting, which will show all current sorting settings.\n\nIf you have multiple sorting methods applied, they will work in the order you applied them (represented in order in the Sort menu). For example, you can sort the ‚Äútitle‚Äù column alphabetically, and then sort by ‚Äúrelease year‚Äù. If you apply those in a different order - sort ‚Äúrelease year‚Äù the dataset first, and then alphabetically by ‚Äútitle‚Äù - your dataset will look different.\n\nSorting by multiple columns\nYou can sort by multiple columns by performing sort on additional columns. The sort will depend on the order in which you select columns to sort. To restart the sorting process with a particular column, check the sort by this column alone box in the Sort pop-up menu.\nIf you go back to one of the already sorted columns and select &gt; Sort &gt; Remove sort, that column is removed from your multiple sort. If it is the only column sorted, then data reverts to its original order.",
    "crumbs": [
      "Filtering & Sorting"
    ]
  },
  {
    "objectID": "filtering.html#find-replacing",
    "href": "filtering.html#find-replacing",
    "title": "Filtering & Sorting",
    "section": "Find & Replacing",
    "text": "Find & Replacing\nJust like in other data tools, OpenRefine includes a powerful ‚ÄúFind and Replace‚Äù feature. For example, let‚Äôs say we‚Äôre working with the country column. Creating a text facet on this column might result in over 1,000 unique entries‚Äîfar too many to scan manually for inconsistencies.\nSuppose you notice that some values use ‚ÄúUSA‚Äù instead of the expected two-character code ‚ÄúUS‚Äù, based on the ISO 3166-1 alpha-2 standard. In that case, you can use Find and Replace to correct these entries and ensure consistency across your dataset.\nSince the country may be part of a string with other countries we could use the Edit cells &gt; Replace...\n\nThat approach would solve the issue‚Äîbut only because we already knew exactly what the problem was and which 22 cells needed to be fixed. However, having the data in this column stored as a single string makes it much harder to explore and analyze effectively. That‚Äôs why splitting the data is important. Additionally, we can apply more advanced strategies to identify anomalies‚Äîsuch as checking whether all country codes follow the recommended two-character format‚Äîand flag any cells that need attention. We‚Äôll dive deeper into these techniques in the upcoming episodes.",
    "crumbs": [
      "Filtering & Sorting"
    ]
  },
  {
    "objectID": "saving.html",
    "href": "saving.html",
    "title": "Saving & Sharing Things",
    "section": "",
    "text": "In this episode, we will explore how to document and export your data cleaning workflows in OpenRefine. As seen, as data projects grow more complex, keeping track of each transformation becomes essential‚Äînot just for your own reference, but also for collaborating with others and ensuring reproducibility.\nOpenRefine offers powerful features that allow you to capture every step of your data cleaning process in a structured format. You‚Äôll learn how to export this process as a reusable script and share both your cleaned dataset and the workflow that produced it. This ensures that your work can be replicated, audited, or continued by colleagues or future you.\nAs previously mentioned you can export your clean dataset or subsets of the cleaned dataset using the Export menu and choosing you preferred extension/format. As a reminder the raw data was never touched,\nOne of OpenRefine‚Äôs key strengths is that the raw data remains untouched, while all changes made to the working copy are meticulously tracked‚Äîexcept for modifications to project metadata, such as the project name. By this point, anyone following along should have a fairly extensive list of actions recorded in the Undo/Redo panel, beginning from the project‚Äôs creation.\n\nClicking Extract.. should prompt the extract operating history, which is essentially a compilation of all your cleaning actions in a JSON (JavaScript Object Notation) file, which stores data in a structured, human-readable text format using key-value pairs and arrays.\n\nYou can choose to export specific actions or the entire file. This export is useful not only for reapplying edits to similar datasets, but also for enhancing transparency and reproducibility by clearly documenting the transformation from raw to analysis-ready data. You may reuse operations, by clicking Apply... and uploading the file.\nLet‚Äôs see how does that work? Export the file and save it. Then, go to the Undo/Redo panel and go back to the step where the project was created and apply the file with all the actions you have performed. Cool, right?",
    "crumbs": [
      "Saving & Sharing Things"
    ]
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Creating a New Project",
    "section": "",
    "text": "Now that OpenRefine is up and running‚Äîand we‚Äôve got our dataset ready‚Äîit‚Äôs time to create a new project and start exploring what this powerful tool can do.\nWhen you launch OpenRefine by clicking the diamond logo or accessing http://127.0.0.1:3333 you‚Äôll presented with a home screen with some options, among which to start a new project. Let‚Äôs explore some of these options together before proceeding:\n\n\n\n\n\n\n\nAdjusting Display Settings\n\n\n\n\n\nHave you noticed the Preferences link in the lower-left corner? If you click on it, you can adjust settings that apply to all projects. Since we‚Äôll be working with a dataset containing over 25,000 rows, let‚Äôs make sure a few things will be properly displayed and change the parameter key ui.browsing.listFacet.limit to 30,000.\nFor more info about system preferences: https://openrefine.org/docs/manual/running#preferences\n\n\n\nAlright, back to creating a project, OpenRefine allows users to choose files from a local machine by uploading XLS/XLSX, CSV, TSV, TXT, XML, JSON files and more, fetch data directly from a website such as Electric Vehicle Population Data from Data.gov, connect it with a database through login and authentication, or paste data directly from clipboard.\nWe will be using the first option and select the example dataset. This should prompt an auto preview of the dataset, for a quick inspection of the first rows, and with some configuration and parsing options, including character encoding (see handout for more info on this topic). OpenRefine automatically detects the character encoding used in your file.\n\nWe‚Äôll stick with the default settings for now‚Äîthey typically work well for most CSV datasets. However, if the data preview doesn‚Äôt look quite right, it may be worth adjusting the parsing options to better match your file‚Äôs structure.\nKeep in mind that OpenRefine does not retain any original formatting from your file. Elements like cell colors, font styles, or background shading will be lost during import. Hyperlinked text will appear as plain text, though OpenRefine will detect any URLs and make them clickable within the project interface.\nThat said, relying on visual formatting or emphasis‚Äîlike colors or bold text‚Äîto convey important meaning isn‚Äôt recommended in data management. These elements aren‚Äôt machine-readable and can lead to inconsistencies or misinterpretation during analysis. It‚Äôs always better to encode meaning directly in the data using clearly labeled columns or consistent values.\nIf your spreadsheet includes multiple worksheets, OpenRefine will display each one along with its row count. You‚Äôll need to choose a single worksheet to import‚Äîonly one can be selected at a time.\n\nIn the upper-right corner, you can rename your project and add tags to link it with related projects. Once you‚Äôre set, simply click Create Project Now that we‚Äôve created our project, let‚Äôs take a step back and explore a few more features. Click the OpenRefine logo on the left-hand side and select Open Project Great! You should see the project we just created. By clicking on About you can edit the project metadata, helping keep everything organized and easy to locate. Experiment yourself.\n\nIf you scroll down, you‚Äôll see the Project ID assigned to your new project in the metadata panel‚Äînice work! Now that everything‚Äôs set up, let‚Äôs jump back into the project and explore some of OpenRefine‚Äôs powerful features. To return to your project, just click on its name.",
    "crumbs": [
      "Creating a New Project"
    ]
  },
  {
    "objectID": "project.html#getting-things-started",
    "href": "project.html#getting-things-started",
    "title": "Creating a New Project",
    "section": "",
    "text": "Now that OpenRefine is up and running‚Äîand we‚Äôve got our dataset ready‚Äîit‚Äôs time to create a new project and start exploring what this powerful tool can do.\nWhen you launch OpenRefine by clicking the diamond logo or accessing http://127.0.0.1:3333 you‚Äôll presented with a home screen with some options, among which to start a new project. Let‚Äôs explore some of these options together before proceeding:\n\n\n\n\n\n\n\nAdjusting Display Settings\n\n\n\n\n\nHave you noticed the Preferences link in the lower-left corner? If you click on it, you can adjust settings that apply to all projects. Since we‚Äôll be working with a dataset containing over 25,000 rows, let‚Äôs make sure a few things will be properly displayed and change the parameter key ui.browsing.listFacet.limit to 30,000.\nFor more info about system preferences: https://openrefine.org/docs/manual/running#preferences\n\n\n\nAlright, back to creating a project, OpenRefine allows users to choose files from a local machine by uploading XLS/XLSX, CSV, TSV, TXT, XML, JSON files and more, fetch data directly from a website such as Electric Vehicle Population Data from Data.gov, connect it with a database through login and authentication, or paste data directly from clipboard.\nWe will be using the first option and select the example dataset. This should prompt an auto preview of the dataset, for a quick inspection of the first rows, and with some configuration and parsing options, including character encoding (see handout for more info on this topic). OpenRefine automatically detects the character encoding used in your file.\n\nWe‚Äôll stick with the default settings for now‚Äîthey typically work well for most CSV datasets. However, if the data preview doesn‚Äôt look quite right, it may be worth adjusting the parsing options to better match your file‚Äôs structure.\nKeep in mind that OpenRefine does not retain any original formatting from your file. Elements like cell colors, font styles, or background shading will be lost during import. Hyperlinked text will appear as plain text, though OpenRefine will detect any URLs and make them clickable within the project interface.\nThat said, relying on visual formatting or emphasis‚Äîlike colors or bold text‚Äîto convey important meaning isn‚Äôt recommended in data management. These elements aren‚Äôt machine-readable and can lead to inconsistencies or misinterpretation during analysis. It‚Äôs always better to encode meaning directly in the data using clearly labeled columns or consistent values.\nIf your spreadsheet includes multiple worksheets, OpenRefine will display each one along with its row count. You‚Äôll need to choose a single worksheet to import‚Äîonly one can be selected at a time.\n\nIn the upper-right corner, you can rename your project and add tags to link it with related projects. Once you‚Äôre set, simply click Create Project Now that we‚Äôve created our project, let‚Äôs take a step back and explore a few more features. Click the OpenRefine logo on the left-hand side and select Open Project Great! You should see the project we just created. By clicking on About you can edit the project metadata, helping keep everything organized and easy to locate. Experiment yourself.\n\nIf you scroll down, you‚Äôll see the Project ID assigned to your new project in the metadata panel‚Äînice work! Now that everything‚Äôs set up, let‚Äôs jump back into the project and explore some of OpenRefine‚Äôs powerful features. To return to your project, just click on its name.",
    "crumbs": [
      "Creating a New Project"
    ]
  },
  {
    "objectID": "project.html#saving-things",
    "href": "project.html#saving-things",
    "title": "Creating a New Project",
    "section": "Saving Things",
    "text": "Saving Things\nOpenRefine automatically records all your actions, which are listed in the Undo/Redo panel we will be exploring more soon. This includes things like flagging and starring rows. However, it does not save facets (which will learn more about), filters, or any visual changes you‚Äôve made‚Äîsuch as the number of visible rows, column sorting, or collapsed columns. A good rule to remember: if it‚Äôs not in the Undo/Redo panel, it won‚Äôt be saved when you exit the project.\nBy default, OpenRefine autosaves your project every five minutes. If you‚Äôd like you can adjust this setting using the command line and following the instructions laid out here.\nWhile you can‚Äôt save general view settings, you can save and share your current facets and filters. To do this, click Permalink. This generates a new URL that reloads the project with your current facets and filters, including specific settings (like sorting by count instead of name). Copy and save that URL to return to the same filtered view later.\n\nWhere are projects stored locally?\nAll of your raw project files are stored in your OpenRefine work directory. Each file is named using the unique Project ID assigned by OpenRefine. You can find this ID by going to the Open Project screen and clicking the About link next to the project. Let‚Äôs take a moment to check where your files are.\nOpenRefine stores data in two places:\n\nProgram files in the program directory, wherever you‚Äôve chose to installed it\nProject files in what we call the workspace directory\n\nYou can access the workspace directory by:\n\nLaunch OpenRefine and click Open Project in the sidebar\nAt the bottom of the screen, click Browse workspace directory\nA file-explorer or finder window will open in your workspace\n\nWe will keep the default configuration for now, but you can customize the desired location by following these instructions.\n\n\nWhat happens to the original dataset?\nOpenRefine gives researchers piece of mind by following a key best practice in data management: always preserve an untouched version of your original dataset. Why? Because having an unaltered copy ensures you can always return to the source if something goes wrong, verify your changes, or reprocess the data with a different approach‚Äîwithout risking data loss or corruption.\n\n\nUpgrades & Backups\nOpenRefine is continuously evolving‚Äîbringing new features, performance upgrades, and sometimes changes to how projects are handled. While these updates improve the tool, they may also introduce compatibility issues with older project files.\nIf you‚Äôre upgrading from an older version of OpenRefine and already have projects saved on your computer, it‚Äôs strongly recommended that you back them up before installing the new version.\nDuring these transitions, to protect your work, we advise you to first locate your workspace directory (this is where all your project files are stored). Then, copy everything in that folder and paste it into a separate backup location on your computer. This simple step ensures that your data remains safe, no matter what changes come with the upgrade.",
    "crumbs": [
      "Creating a New Project"
    ]
  },
  {
    "objectID": "features.html",
    "href": "features.html",
    "title": "Navigating the Basics in OpenRefine",
    "section": "",
    "text": "Before diving into data cleaning and transformation tasks, it‚Äôs important to do a walk-through of OpenRefine‚Äôs interface to get familiar with how to navigate and use its features effectively.",
    "crumbs": [
      "Navigating the Basics in OpenRefine"
    ]
  },
  {
    "objectID": "features.html#interface-walk-through",
    "href": "features.html#interface-walk-through",
    "title": "Navigating the Basics in OpenRefine",
    "section": "Interface Walk-Through",
    "text": "Interface Walk-Through\nUnderstanding the layout, menus, and tools available will help streamline your workflow and make it easier to locate the functions you need.\n\n\nProject Bar\nThe project bar is located at the very top of the screen. It includes the OpenRefine logo, the project title, and a permalink next to it in the left side. This permalink is a special URL that captures the current state of your project‚Äôs facets and filters. It lets you save or share exactly what you‚Äôre looking at‚Äîso if you‚Äôve filtered your data in a certain way or set up specific facets, you can generate a permalink to return to that exact view later or share it with someone else.\nTo create a permalink, click on it and OpenRefine will reload the project with a new URL in the address bar. That URL includes all your current facets, filters, and their specific settings (like sorting order). You can then copy that URL and save or share it with collaborators. Essentially, permalinks are especially useful for collaboration, documentation, or revisiting a specific view of your data. When shared, they allow others to see the exact same project state you were working with‚Äîincluding filtered rows, selected facets, and how the data was sorted. This makes it easier to troubleshoot issues, review changes, or discuss findings with teammates without needing to replicate the setup manually.\nIt is important to note, however that the permalink DOES NOT save your data or transformations‚Äîit only saves how you‚Äôre currently viewing the data. All transformations are still stored separately in the project‚Äôs History.\nYou can return to the home screen at any time by clicking the OpenRefine logo. To open another project in a new tab or window, right-click the logo and select ‚ÄúOpen in a new tab.‚Äù Keep in mind that closing your current project will reset your facets and view settings, though all data transformations will remain saved in the project‚Äôs History.\n\n\n\n\n\n\nRefrain from Using the Browser‚Äôs Back Button\n\n\n\nDon‚Äôt use your browser‚Äôs ‚Äúback‚Äù button ‚Äî it may close your project and erase your facets and view settings.\n\n\n\n\nGrid Header\nThe grid header appears just below the project bar and above the project grid, where your project‚Äôs data is displayed. It shows the total number of rows or records in your project‚Äîin this case, 25,223‚Äîand indicates whether you‚Äôre currently viewing the data in rows or records mode. If you‚Äôre viewing a filtered or faceted subset of the data - let‚Äôs say we select only the records corresponding to shows, the grid header will reflect that, showing something like ‚Äú5,774 matching rows(25,223 total)‚Äù to let you know you‚Äôre seeing a subset of the dataset.\nDirectly beneath the row count, you can toggle between row mode and records mode. OpenRefine stores each project in one of these modes and, by default, opens in records mode if that‚Äôs how the project was saved.\nTo the right of the rows/records toggle, you‚Äôll find controls to adjust how many rows or records are visible on screen at once. On the far right, navigation buttons let you move through your dataset one page at a time.\n\n\nAll Column\nThe first column in every project is always labeled ‚ÄòAll‚Äô which refers to the complete set of data fields (columns) in your dataset. These columns represent the structure of your data, and working with it allow you to perform operations that apply to all collumns. It also displays the numbering of rows or records, which reflects their permanent order. While temporary sorts or filters (facets) may rearrange or limit the visible rows, the numbering continues to show the original identifiers, unless a permanent change is made to the dataset.\nBesides the sequential numbering, this column also has stars and flags. Both of these icons have neutral meaning, and let you mark specific rows based on your own criteria, for later focus. Unlike permalinks, they persist even after closing and reopening your project. You can use them however you like, though they‚Äôre typically used to flag errors or highlight important rows you would like to edit.\nTo manually star or flag a row, just click the star or flag icon in the All column. You can also use All ‚Üí Edit rows ‚Üí Star/Flag rows to tag larger groups at once‚Äîhelpful when working with patterns that need multi-step fixes that break filters. Instead of re-filtering as the data changes, flag the rows first, then use Facet by flag to keep track.\nUsing both stars and flags can help you track two different sets of criteria within your dataset. For example, in our dataset you might consider using stars to highlight titles that are trending or have a high rating (your first filter). Then, after clearing the stars, you could use flags to mark rows‚Äîlike rows 8 and 10‚Äîthat are missing an age classification and need to be reviewed or completed later.\nTitles that meet both criteria (e.g.¬†popular but missing age ratings) will now be both starred and flagged, making them easy to locate using the facet filters. While this approach works, especially in smaller datasets, we‚Äôll explore more efficient methods for managing and updating large streaming libraries later on.\n\n\nReording, Renaming & Deleting Columns\nOpenRefine is primarily designed for data cleaning rather than adding new columns or rows. However, it does provide essential tools for reorganizing your dataset, including renaming, reordering, and deleting columns.\nFor example, we want to remove the imdb_votes and tmdb_votes columns. This can be done in one of two ways: by deleting each column individually or by using the ‚ÄúAll‚Äù column dropdown to manage multiple columns at once. Choose the method that works best for you to exclude them:",
    "crumbs": [
      "Navigating the Basics in OpenRefine"
    ]
  },
  {
    "objectID": "transforming.html",
    "href": "transforming.html",
    "title": "Performing Data Transformations",
    "section": "",
    "text": "Now that we have learned a few things about data types and facets we will dive a little deeper on how we can use them to streamline some transformations to make our data tidy.",
    "crumbs": [
      "Performing Data Transformations"
    ]
  },
  {
    "objectID": "transforming.html#common-transforms",
    "href": "transforming.html#common-transforms",
    "title": "Performing Data Transformations",
    "section": "Common Transforms",
    "text": "Common Transforms\nOpenRefine provides a range of common transformations that help users structure and standardize their datasets. We can perform very simple to more advanced transformations. Besides transforming data types, we can change text cases, trim whitespaces, split and join values and find and replace text and much more.\nFor instance, we can change type values movie and show to lowercase. Or we could apply the collapse consecutive whitespace transformation to the description column. This function will remove all space characters that sit in sequence and replace them with a single space. Let‚Äôs try it and see what we get!",
    "crumbs": [
      "Performing Data Transformations"
    ]
  },
  {
    "objectID": "transforming.html#clustering",
    "href": "transforming.html#clustering",
    "title": "Performing Data Transformations",
    "section": "Clustering",
    "text": "Clustering\nRemember we have several entries for the same streaming service but with some variations? Editing those values for deduplication manually could take us too much time.\nNow, let‚Äôs move to the streaming column. We want to keep six unique values in a simplified choice in title case: Amazon, Apple, Disney, Max, Netflix and Paramount. We can use the Edit cells &gt; Common transforms &gt; To titlecase and we end up with less choices, saving us quite some clicks and edits.\nClustering helps you find and merge similar but inconsistently written values. It‚Äôs super useful when you have messy data ‚Äî think entires like: ‚ÄúNew York‚Äù, ‚Äúnew york‚Äù, ‚ÄúNewYork‚Äù, or ‚ÄúN.Y‚Äù. You know they‚Äôre referring to the same location ‚Äî but the computer doesn‚Äôt, unless you help it out. OpenRefine offers two main methods to resolve these type of issues.\n\nKey Collision Methods\nThese methods simplify the data to make it easier to compare. They look for basic patterns (like removing spaces, turning everything into lowercase, or sorting words alphabetically) to figure out if two things are similar.\nThink of it like simplifying everything to a ‚Äúcore‚Äù version and then checking if the core versions match.\nExample: ‚ÄúApple Inc.‚Äù and ‚ÄúINC. Apple‚Äù will both be simplified to the same ‚Äúcore version‚Äù and grouped together.\n\n\nNearest Neighbor Methods\nThese methods directly compare the values to see how similar they are. They measure how much you‚Äôd need to change one value to turn it into the other (like fixing typos or small differences).\nThink of it like checking each pair of values closely and asking, ‚ÄúHow different are these from each other?‚Äù\nExample: ‚ÄúMicheal‚Äù and ‚ÄúMichael‚Äù are compared and seen as very similar because they only differ by one letter.\n\n\nMost Commonly Used OpenRefine Clustering Methods\n\n\n\n\n\n\n\n\n\n\nMethod\nCategory\nDescription\nGood For\nExample\n\n\n\n\nFingerprint\nKey Collision\nLowercases, removes punctuation, sorts words alphabetically\nGeneral cleanup of similar words\nJohn Smith = Smith John\n\n\nNgram-Fingerprint\nKey Collision\nLike Fingerprint but uses chunks of letters (n-grams)\nCatching typos and small variations\nJon ‚âà John\n\n\nMetaphone3\nKey Collision\nGroups words that sound alike using English phonetics\nNames, brand names\nKatherine ‚âà Catherine\n\n\nLevenshtein Distance\nNearest Neighbor\nCounts how many single-character edits are needed to match two strings\nShort strings with typos\nJon ‚âà John, color ‚âà colour\n\n\nPPM (Prediction by Partial Matching)\nNearest Neighbor\nUses letter patterns to guess similarity based on character prediction\nVery short strings, abbreviations\nNY ‚âà New York (sometimes)\n\n\n\nIn the facet we have open for the column streaming, click Cluster to check available options. It should prompt the following:\n\nOpenRefine initially identified 6 clusters using the default method. However, switching to the Nearest Neighbor clustering method with the Levenshtein distance function would be more effective in capturing clusters that reflect spelling variations present in our dataset.\n\nAwesome! OpenRefine has identified 13 clusters and provides some helpful options to easily harmonize and merge them. You can select the clusters you want to merge and enter the desired standardized value‚Äîfor example: Amazon, Apple, Disney, Max, Netflix, and Paramount.\nOnce you‚Äôre ready, click Merge Selected & Re-cluster rather than Merge & Close. We recommend this because re-clustering can help OpenRefine detect additional clusters you might want to merge.\n\nThis step should reduce the total number of distinct values to 9. However, there will still be some uncleaned entries remaining for three of the streaming services.\n\n\n\n\n\n\nüß† Your Turn! Exploring Clustering Options\n\n\n\n\n\nNavigate the same method Nearest Neighbor and check which options could help the clustering for Amazon.\n\n\n\n\n\n\nüîë Answer Key\n\n\n\nThis method uses a parameter called radius which represents a distance threshold and controls how close or similar things have to be in order to be considered a match. The default is set as 3.0 but if we increase it to 6.0 it will be less strict and find more potential matches. For example:\n‚ÄúJon‚Äù and ‚ÄúJohn‚Äù might match at 6.0, but not at 3.0.\n‚ÄúJon‚Äù and ‚ÄúJonathan‚Äù would likely only match with a larger radius like 6.0.\nFor our Amazon example it will work just fine since we don‚Äôt other competing streaming services with similar names. Now that we have stretched the radius, let‚Äôs merge those clusters accordingly.\n\n\n\n\n\n\nWait, we are not ready just yet, notice that we have two entries for the same streaming service HBO and Max. In this case we need to manually change them, since it is a easy fix, and no clustering method would be able to pair these values accordingly.\n\n\n\n\n\n\nImportant\n\n\n\nWhile we could spend much more time delving into the variety of clustering methods and their nuanced differences, that level of detail falls into more advanced territory. The best approach often depends heavily on the nature of your data and the specific goals of your analysis. For those interested in exploring further, we recommend reviewing the official documentation of the clustering algorithms you‚Äôre considering and consult with us if you have any questions.",
    "crumbs": [
      "Performing Data Transformations"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops\n\n\n\n\n\n\n\n\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu ) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.",
    "crumbs": [
      "About Us"
    ]
  },
  {
    "objectID": "about.html#our-department",
    "href": "about.html#our-department",
    "title": "About Us",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops\n\n\n\n\n\n\n\n\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu ) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.",
    "crumbs": [
      "About Us"
    ]
  },
  {
    "objectID": "about.html#using-this-resource",
    "href": "about.html#using-this-resource",
    "title": "About Us",
    "section": "Using this Resource",
    "text": "Using this Resource\nThis workshop is licensed CC-BY. We welcome everyone to take advantage of this learning resource asynchronously at their own pace. We also encourage faculty to reuse or repurpose content to their research methods classes. If you‚Äôd like us to host a hands-on in-person or remote workshop for your team or schedule a consultation with us, email: rds@library.ucsb.edu.",
    "crumbs": [
      "About Us"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidying Messy Spreadsheets with OpenRefine",
    "section": "",
    "text": "Real-world datasets are rarely neat. They often come with inconsistencies, errors, missing values, and formatting issues that can throw off analysis and lead to misleading conclusions. That‚Äôs why planning your data preprocessing steps in advance is so important‚Äîit allows you to anticipate common challenges and set up strategies to standardize formats, handle missing data, and resolve discrepancies efficiently.\nThis process, known as data cleaning‚Äîor sometimes data wrangling, harmonization, or refinement‚Äîis essential to any data-driven project. It involves identifying and fixing issues like missing values, duplicates, formatting problems, and outliers. With thoughtful planning, data cleaning can dramatically improve the quality and reliability of your data, leading to clearer insights and better decisions.\nWe all love the idea of working with clean, reliable data‚Äîit‚Äôs what makes everything else possible. But when it comes to actually doing the cleaning? Well‚Ä¶ that‚Äôs a different story.\n\nSource: Adapted from Schmarzo (2021).\nThomas Edison once said, ‚ÄúGenius is 1% inspiration, 99% perspiration.‚Äù And guess what? For most projects involving data, a big part of the perspiration can be attributed to data cleaning. It‚Äôs not glamorous, but without it, great findings could not see the light of day.\nEven though it‚Äôs a critical part of the workflow‚Äîand often takes up about 60% of a data professional‚Äôs time1‚Äîdata cleaning is sometimes seen as tedious or less exciting. But in truth, it‚Äôs the foundation of trustworthy analysis. Without clean data, even the most advanced models or tools won‚Äôt deliver meaningful results. In short: clean data, better outcomes.\nThough data cleaning may not be as gratifying as deriving insights from analysis, it is a crucial and often time-consuming task. Ensuring that data is accurate, consistent, and error-free requires considerable effort, particularly when dealing with large datasets where even small inconsistencies can lead to flawed results. The attention to detail required for this task makes data cleaning one of the most labor-intensive aspects of data analysis.\nAlthough the cleaning process may vary depending on the dataset, the following tips will help you streamline this task and improve the consistency, accuracy, and quality of your data.\n\n\nSource: UCSB Library Data Literacy Series. Click to expand: (perma.cc/HE29-3QVH).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#data-cleaning-a-foreword",
    "href": "index.html#data-cleaning-a-foreword",
    "title": "Tidying Messy Spreadsheets with OpenRefine",
    "section": "",
    "text": "Real-world datasets are rarely neat. They often come with inconsistencies, errors, missing values, and formatting issues that can throw off analysis and lead to misleading conclusions. That‚Äôs why planning your data preprocessing steps in advance is so important‚Äîit allows you to anticipate common challenges and set up strategies to standardize formats, handle missing data, and resolve discrepancies efficiently.\nThis process, known as data cleaning‚Äîor sometimes data wrangling, harmonization, or refinement‚Äîis essential to any data-driven project. It involves identifying and fixing issues like missing values, duplicates, formatting problems, and outliers. With thoughtful planning, data cleaning can dramatically improve the quality and reliability of your data, leading to clearer insights and better decisions.\nWe all love the idea of working with clean, reliable data‚Äîit‚Äôs what makes everything else possible. But when it comes to actually doing the cleaning? Well‚Ä¶ that‚Äôs a different story.\n\nSource: Adapted from Schmarzo (2021).\nThomas Edison once said, ‚ÄúGenius is 1% inspiration, 99% perspiration.‚Äù And guess what? For most projects involving data, a big part of the perspiration can be attributed to data cleaning. It‚Äôs not glamorous, but without it, great findings could not see the light of day.\nEven though it‚Äôs a critical part of the workflow‚Äîand often takes up about 60% of a data professional‚Äôs time1‚Äîdata cleaning is sometimes seen as tedious or less exciting. But in truth, it‚Äôs the foundation of trustworthy analysis. Without clean data, even the most advanced models or tools won‚Äôt deliver meaningful results. In short: clean data, better outcomes.\nThough data cleaning may not be as gratifying as deriving insights from analysis, it is a crucial and often time-consuming task. Ensuring that data is accurate, consistent, and error-free requires considerable effort, particularly when dealing with large datasets where even small inconsistencies can lead to flawed results. The attention to detail required for this task makes data cleaning one of the most labor-intensive aspects of data analysis.\nAlthough the cleaning process may vary depending on the dataset, the following tips will help you streamline this task and improve the consistency, accuracy, and quality of your data.\n\n\nSource: UCSB Library Data Literacy Series. Click to expand: (perma.cc/HE29-3QVH).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#important-considerations",
    "href": "index.html#important-considerations",
    "title": "Tidying Messy Spreadsheets with OpenRefine",
    "section": "Important Considerations",
    "text": "Important Considerations\nEffective data cleaning begins with thorough assessment, preparation and planning. By identifying potential issues, setting clear protocols, and ensuring reproducibility, researchers can maximize the value of their data while maintaining accuracy, compliance, and efficiency.\nWithout proper planning, aggressive cleaning methods can inadvertently remove valuable information. Establishing clear criteria for data retention, transformation, and exclusion helps maintain the dataset‚Äôs integrity while eliminating irrelevant or misleading entries.\nBecause data cleaning cannot always be an entirely objective task, Rawson & Mu√±oz (2016)2 caution researchers to prioritize index-making strategies rather than immediately jumping into normalization. In this approach, the first step is to identify the communities in which the data holds meaning, clarify any questions related to the datset, and to explore the concepts that shape and structure the data. Then, have all decisions made along the way documented.\nA common data cleaning mistake is relying too heavily on manual processes and documentation, which are not only time-consuming but also highly prone to human error. This is where automation tools come in‚Äîstreamlining the cleaning process and significantly improving accuracy.\nWhile leveraging programmatic techniques, data cleaning can be more efficient, scalable, and reproducible, especially when working with large datasets. The good news is that non-coders can also take advantage of tools to support a more reproducible data cleaning workflow without losing track of changes and required transformations along the way.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Tidying Messy Spreadsheets with OpenRefine",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces data cleaning best practices with the support of OpenRefine, a powerful open-source and non-programmer friendly tool designed to clean, transform, and structure messy datasets with efficiency and precision. By learning how to identify and resolve common data quality issues, you will gain the skills needed to ensure that your spreadsheets are reliable, standardized, and ready for deeper analysis. Whether you‚Äôre working with survey results, bibliographic records, recorded observations, financial reports, or any other tabular data, mastering data-cleaning techniques will enhance the integrity and reusability of your datasets.\nThrough hands-on activities we will explore how OpenRefine helps detecting anomalies, streamlines data cleaning, and allows for more consistent and reproducible workflows. By the end of this workshop, you will be equipped with practical skills to tidy and refine tabular datasets, enabling more accurate and meaningful data-driven work in your field.\nWe also encourage learners to also explore our handouts from our Data Literacy Series with practical recommendations on related topics, including: handling missing data, dealing with encoding issues, keeping spreadsheets organized and control for quality in tabular data.\n\nLearning Goals\nIn this hands-on workshop, you‚Äôll learn how to efficiently clean, transform, and structure messy spreadsheets using OpenRefine, a powerful open-source tool for data wrangling. Whether you‚Äôre tackling inconsistencies, duplicates, or complex formatting challenges, OpenRefine enables you to automate and streamline the process, ensuring greater accuracy and consistency in your datasets.\nThroughout this session, we will explore how to:\n\nAnalyze and diagnose data issues using facets, clustering, and structured views to identify inconsistencies.\nFilter and refine data by removing irrelevant observations and keeping only records that meet specific criteria.\nStandardize and reconcile values to maintain consistent spelling and formatting across entries, leveraging OpenRefine‚Äôs clustering and reconciliation features.\nTransform and manipulate data using built-in transformations and the General Refine Expression Language (GREL) for advanced modifications.\nDocument and export workflows to ensure reproducibility and share cleaned datasets with collaborators.\n\nBy applying facets, sorting, clustering, GREL expressions, and reconciliation, you‚Äôll gain hands-on experience in making messy data more structured, usable, and analysis-ready. Whether you‚Äôre a researcher, librarian, or data professional, this workshop will equip you with practical skills to enhance your data management workflow.\n\n\nSchedule\n\n\n\nTime\nTopic\n\n\n\n\n09:30 AM\nWelcome & Introductions\n\n\n09:40 AM\nSetup & Installation\n\n\n10:00 AM\nOur Running Example\n\n\n10:10 AM\nCreating a New Project\n\n\n10:20 AM\nNavigating the Basics in OpenRefine\n\n\n10:30 AM\nExploring Data Types & Facets\n\n\n10:50 AM\nBreak\n\n\n11:00 AM\nFiltering & Sorting\n\n\n11:10 PM\nPerforming Data Transformations\n\n\n11:20 PM\nMore Transformations with GREL\n\n\n11:30 PM\nSaving & Sharing Things + Wrap-up",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Tidying Messy Spreadsheets with OpenRefine",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAbhigyan. (2020, April 26). Why is ‚ÄúDATA CLEANING‚Äù necessary? Analytics Vidhya. https://medium.com/analytics-vidhya/why-is-data-cleaning-neccessary-94f2b2b01e9d‚Ü©Ô∏é\nRawson, K., & Mu√±oz, T. (2016, July 7). Against cleaning. Curating Menus. http://www.curatingmenus.org/articles/against-cleaning‚Ü©Ô∏é",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "exploring.html",
    "href": "exploring.html",
    "title": "Exploring Data Types & Facets",
    "section": "",
    "text": "Now, let‚Äôs dive into one of the most powerful and defining features of OpenRefine‚Äîfacets‚Äîwhich also explains why its logo is shaped like a diamond.\nFaceting is a method of exploring and filtering data to better understand its structure and content. It allows us to more easily spots errors and outliers in the data. By applying multiple filters, you can quickly uncover patterns, spot inconsistencies, and isolate specific subsets of data for closer inspection or bulk editing. A facet groups together all the similar values within a column, allowing you to easily filter and refine your dataset. It‚Äôs also incredibly useful for editing values across many records at once.",
    "crumbs": [
      "Exploring Data Types & Facets"
    ]
  },
  {
    "objectID": "exploring.html#text-facets",
    "href": "exploring.html#text-facets",
    "title": "Exploring Data Types & Facets",
    "section": "Text Facets",
    "text": "Text Facets\nOne type of facet is called a ‚ÄòText facet‚Äô. This groups all the identical text values in a column and lists each value with the number of records it appears in. The facet information always appears in the left hand panel in the OpenRefine interface.\nAfter applying a text facet to the title column, you‚Äôll notice that the number of unique choices differs from the total number of rows. This happens because a single title may appear across multiple streaming platforms, and also because different productions can share the same name. Let‚Äôs sort the facet by count to explore how often titles like Cinderella appear. Just a heads-up: we don‚Äôt have any duplicate title entries to clean up in this dataset.\n\n\n\n\n\n\n\nAs you can see, a text facet can come in handy for providing some quick insights into patterns, and spotting potential inconsistencies or duplicates across your dataset.\n\nChecking For Errors\nLet‚Äôs now turn our attention to the classification column. This column should represent the ratings from The Classification and Rating Administration (CARA). How many unique values are represented in the dataset? You should find 18 distinct entries‚Äîcorrect? It appears that the original data collector could have enforced validation rules to ensure consistency in this field, but since that wasn‚Äôt done, we can use faceted analysis to identify inconsistencies or errors. How many could you identify? Have you noticed the question marks and NAs? What approach would you take to clean and standardize these entries?\nSince those entries represent either non-existent data or uncertainty‚Äîand given that we already have many productions with unknown classifications (13,132)‚Äîwe might consider setting those values to blank as well. To do this, we can first run a text facet and select the entries we want to amend:\n\nNow that we have included those six choices and eight entries, we can apply a `common transform` and set those values as nulls.\n\nNow, let‚Äôs run another text facet for the same column and check how many choices we have. But wait, have you noticed the empty string option? An empty string will work the same as null for most purposes. Can you guess when it will not be the case? An empty string (\"\") will not behave the same as null when you use functions or filters that specifically distinguish between null and empty. Setting it as null represent ‚Äúno data‚Äù, making it more explicit the data is missing, unknown or not applicable. But let‚Äôs not get too sidetracked. We will cover more about transformations in the next episodes.\nNow we will use faceting to look for potential errors in data entry in the streaming column. First, Scroll over to the streaming column and then, click the down arrow and choose Facet &gt; Text facet.\n\nAlright, in the left panel, you should now see a box containing every unique value in the streaming column along with a number representing how many times that value occurs in the column.\n\nSort the facet by both name and count. Do you spot any issues with the data? What are they?\nNext, hover over one of the names in the facet list. You should see an option to edit it.\nFor example, you can click edit right next to Apple tv or even include other multiple choices and edit it manually to reconcile them all as one value Apple. As you can see, you can use this feature to correct the error right away. OpenRefine will then prompt you to apply the same fix to all matching values. However, OpenRefine offers even more advanced methods to identify and correct errors, which we‚Äôll explore in the next section when we discuss clustering.\n\n\n\n\n\n\nüß† Your Turn! Applying Text Facet\n\n\n\n\n\n\nUsing faceting, find out what is the oldest production included in this dataset.\nWhich year accumulate the most productions?\nWait‚Äîwhy did we apply a text facet to this column in the first place? Is it actually formatted as a number, a date, or plain text?\n\n\n\n\n\n\n\nüîë Answer Key\n\n\n\n\n\n1.  1901\n2.  2021\n3.  By default all columns in OpenRefine are formatted as a string of text. To ensure you want it to treat it as another type, you will need to transform values accordingly.\n\n\n\n\n\n\nBy the way, when working with facets, the left panel can start to look a bit cluttered. That‚Äôs perfectly normal‚Äîfacets are not the end goal, but rather a tool to help guide transformations, as we‚Äôll explore in the next lesson. You can close facets at any time and re-run them later if needed, or simply adjust their size to keep your workspace manageable.",
    "crumbs": [
      "Exploring Data Types & Facets"
    ]
  },
  {
    "objectID": "exploring.html#numeric-facets",
    "href": "exploring.html#numeric-facets",
    "title": "Exploring Data Types & Facets",
    "section": "Numeric Facets",
    "text": "Numeric Facets\nWhen you import a table into OpenRefine, all columns are initially automatically treated as text, representing a string of sequential characters.\nOnly observations that include only numerals (0-9) can be transformed to numbers. If you apply a number transformation to a column that doesn‚Äôt meet this criteria, and then click the Undo / Redo tab, you will see a step that starts with Text transform on 0 cells. This means that the data in that column was not transformed.\nFor columns with numeric values we can convert it using the Edit cells &gt; Common transforms. In this section, we‚Äôll experiment with converting columns to numbers and explore the additional features and functionality this unlocks.\nSometimes there are non-number values or blanks in a column which may represent errors in data entry and we want to find them. We can do that with a Numeric facet. So if you try to create a numeric facet for the column release_year. The facet will be empty because OpenRefine sees those values as text strings.\nTo transform cells into numbers, click the down arrow for that column, then Edit cells &gt; Common transforms‚Ä¶ &gt; To number. You will notice the values will change from left-justified to right-justified, and black to green color.\n\n\n\n\n\n\nüß† Your Turn! Applying Numeric Facet\n\n\n\n\n\nThe dataset included other numeric columns, but let‚Äôs focus on imdb_score and tmdb_score. How does changing the format for these two column change their faceting display?\n\n\n\n\n\n\nüîë Answer Key\n\n\n\n\n\nPerforming a numeric facet will display a histogram of the number of entries in those columns.",
    "crumbs": [
      "Exploring Data Types & Facets"
    ]
  },
  {
    "objectID": "exploring.html#date-type",
    "href": "exploring.html#date-type",
    "title": "Exploring Data Types & Facets",
    "section": "Date Type",
    "text": "Date Type\nThe ‚Äúdate‚Äù type in OpenRefine is created when a column is explicitly transformed into dates‚Äîeither by applying a built-in expression, using Edit cells ‚Üí Common transforms ‚Üí To date, or manually setting individual cells to the ‚Äúdate‚Äù data type.\nLet‚Äôs take the release_year column as an example. When you apply the To date transformation, OpenRefine attempts to convert each cell into a standardized date format. It uses the ISO 8601 extended format with time in UTC, which looks like this: YYYY-MM-DDTHH:MM:SSZ\nIf the original values are just four-digit years, like:\n1999\n2005\n2012\nOpenRefine will interpret them as full dates by assuming January 1st as the default day and month. So, they‚Äôll be transformed into:\n1999-01-01T00:00:00Z\n2005-01-01T00:00:00Z\n2012-01-01T00:00:00Z\nThis default behavior allows OpenRefine to handle partial date inputs consistently, making it easier to sort, filter, and analyze date-based data.\n\n\n\n\n\n\nüß† Your Turn! Convert release_year to Date Type\n\n\n\n\n\nBack to the release_year column ‚Äî let‚Äôs change it to date type.\nCan you create a numeric or a scatter plot facet?\n\n\n\n\n\n\nüîë Answer Key\n\n\n\n\n\nAttempting to perform a numeric or a scatter plot facet will throw an error because, despite having numbers, the data type is not set as numeric.\n\n\n\n\n\n\nWe‚Äôve covered the most commonly used facets in OpenRefine, but the platform also offers a range of advanced and custom facets that may be relevant to your specific project needs. We encourage you to explore the OpenRefine documentation for a deeper dive into these features.",
    "crumbs": [
      "Exploring Data Types & Facets"
    ]
  },
  {
    "objectID": "exploring.html#facets-for-subsetting-working-dataset",
    "href": "exploring.html#facets-for-subsetting-working-dataset",
    "title": "Exploring Data Types & Facets",
    "section": "Facets for Subsetting Working Dataset",
    "text": "Facets for Subsetting Working Dataset\nFacets can be also very handy to subset the dataset and make it more easily manageable. Let‚Äôs say you want to focus only on the shows for a while, or even export this subset. How would you do that? You can perform a text facet for the column type and select show, you will notice the number of matching rows in the grid header will change accordingly. From now on, you will be only working with those rows, unless you click revert or close the facet panel.\nIf you want to export that subset of shows click export on the right side of the project bar and select your preferred format. As a reminder, the permalink will save all active facets for your project!",
    "crumbs": [
      "Exploring Data Types & Facets"
    ]
  }
]